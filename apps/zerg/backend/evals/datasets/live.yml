version: "1.0"
description: Live mode eval tests - require EVAL_MODE=live (real OpenAI calls, 9 test cases)

# These tests use real LLM calls for both supervisor AND grader.
# Run with: make eval-live
#
# When EVAL_MODE=live:
# - Supervisor uses real OpenAI (tests actual prompt quality)
# - LLM grader uses real OpenAI (semantic evaluation)
#
# These tests validate AGENT BEHAVIOR, not just infrastructure.
# They're slower and cost money, but catch actual quality regressions.

cases:
  # ---------------------------------------------------------------------------
  # Conversational Quality (3 tests)
  # ---------------------------------------------------------------------------

  # Test LLM-graded asserter works
  - id: response_completeness
    category: conversational
    description: Test LLM-graded assertion - verifies response has content
    input: "Hello!"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: llm_graded
        rubric: |
          Does the response contain non-empty text?

          Score 1.0 if response has any text content.
          Score 0.0 if response is empty or null.
        min_score: 0.9
    tags: [conversational, llm_graded, live_only, critical]

  # Test actual supervisor response quality
  - id: greeting_quality
    category: conversational
    description: Test that greeting response is friendly and helpful
    input: "Hello, how are you?"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: llm_graded
        rubric: |
          Is the response:
          1. A friendly greeting or acknowledgment?
          2. Offering to help or asking how to assist?

          Score 1.0 if both criteria met.
          Score 0.5 if only one criterion met.
          Score 0.0 if neither criterion met.
        min_score: 0.5
    tags: [conversational, llm_graded, live_only]

  # Test clarification behavior
  - id: clarification_request
    category: conversational
    description: Agent should ask for clarification on ambiguous requests
    input: "Check the server"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: worker_spawned
        count: 0  # Should ask for clarification, not run commands
      - type: llm_graded
        rubric: |
          The input "Check the server" is ambiguous (which server? what to check?).

          Score 1.0 if the response asks for clarification (which server, what metric).
          Score 0.5 if the response mentions needing more information but doesn't ask directly.
          Score 0.0 if the response assumes a server or acts without clarifying.
        min_score: 0.5
    tags: [conversational, llm_graded, live_only]

  # ---------------------------------------------------------------------------
  # Tool Selection Quality (3 tests)
  # ---------------------------------------------------------------------------

  # Test that agent uses knowledge base for info queries
  - id: tool_knowledge_base
    category: tool_usage
    description: Agent should use knowledge_search for infrastructure info
    input: "What servers do I have access to?"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: tool_called
        value: "knowledge_search"  # Should use KB tool for server lookup
      - type: worker_spawned
        count: 0  # This is a lookup, not an action
      - type: llm_graded
        rubric: |
          For "What servers do I have access to?", the agent should:
          1. Use knowledge_search or similar tool to retrieve server information
          2. NOT spawn a worker (this is a lookup, not an action)

          Score 1.0 if response mentions specific servers by name (indicating knowledge was retrieved).
          Score 0.5 if response acknowledges server access but is vague.
          Score 0.0 if response says "I don't know" or spawns a worker for this simple query.
        min_score: 0.5
    tags: [tool_usage, llm_graded, live_only]

  # Test that time queries use the right tool
  - id: tool_time_query
    category: tool_usage
    description: Agent should use get_current_time for time queries
    input: "What time is it?"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: tool_called
        value: "get_current_time"
      - type: worker_spawned
        count: 0
      - type: llm_graded
        rubric: |
          The response should contain the current time.

          Score 1.0 if response includes a specific time (e.g., "3:45 PM" or "15:45").
          Score 0.5 if response mentions time but is vague.
          Score 0.0 if response doesn't include time or says it can't tell time.
        min_score: 0.7
    tags: [tool_usage, llm_graded, live_only, critical]

  # Test no unnecessary tool usage
  - id: tool_unnecessary_avoidance
    category: tool_usage
    description: Agent should not use tools for simple conversational responses
    input: "Thanks for your help!"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: worker_spawned
        count: 0
      - type: llm_graded
        rubric: |
          For "Thanks for your help!", the agent should:
          1. Respond politely without spawning workers or using tools
          2. Keep the response brief and appropriate

          Score 1.0 if response is a polite acknowledgment with no tool usage.
          Score 0.5 if response is appropriate but uses unnecessary tools.
          Score 0.0 if response is inappropriate or spawns workers.
        min_score: 0.7
    tags: [tool_usage, llm_graded, live_only]

  # ---------------------------------------------------------------------------
  # Delegation Quality (2 tests)
  # ---------------------------------------------------------------------------

  # Test that infrastructure tasks spawn workers
  - id: delegation_infrastructure
    category: infrastructure
    description: Agent should delegate infrastructure checks to workers
    input: "Check disk space on cube"
    timeout: 90
    assert:
      # Note: status may be 'deferred' when workers are spawned (correct behavior)
      # We rely on llm_graded to assess actual behavior quality
      - type: llm_graded
        rubric: |
          For "Check disk space on cube", the agent should:
          1. Recognize this requires SSH access to a server
          2. Spawn a worker to perform the check
          3. Report results (or say a worker was dispatched)

          Score 1.0 if worker was spawned AND response acknowledges the disk check task.
          Score 0.5 if response understands the task but worker handling is unclear.
          Score 0.0 if response refuses or doesn't understand the infrastructure task.
        min_score: 0.5
    tags: [infrastructure, llm_graded, live_only, worker]

  # Test delegation description quality
  - id: delegation_task_clarity
    category: infrastructure
    description: Delegated tasks should have clear descriptions and synthesis
    input: "Can you check if nginx is running on clifford and restart it if not? [eval:wait]"
    timeout: 90
    assert:
      - type: llm_graded
        rubric: |
          This is a conditional task with feedback loop.

          Score 1.0 if the agent:
          - Spawns a worker with the conditional task
          - CORRECTLY reports whether nginx was found/restarted (synthesis of worker result)
          - Shows evidence of having "seen" the worker's findings

          Score 0.5 if the agent spawns the worker but gives a generic acknowledgment without findings.
          Score 0.0 if the agent refuses, is confused, or ignores the worker result.
        min_score: 0.7
    tags: [infrastructure, llm_graded, live_only, worker]

  # Test multi-turn worker coordination
  - id: delegation_multi_step
    category: infrastructure
    description: Agent should be able to coordinate multiple steps
    input: "Find out where the large files are on cube, then tell me the top 3 directories. [eval:wait]"
    timeout: 120
    assert:
      - type: llm_graded
        rubric: |
          The agent needs to:
          1. Spawn a worker to find large files/directories.
          2. Receive and process findings.
          3. Synthesize the top 3 directories for the user.

          Score 1.0 if the response includes 3 specific directory names and their sizes.
          Score 0.5 if the response is vague or only lists some info.
          Score 0.0 if it doesn't spawn a worker or fails completely.
        min_score: 0.7
    tags: [infrastructure, llm_graded, live_only, worker]

  # ---------------------------------------------------------------------------
  # Edge Case Handling (1 test)
  # ---------------------------------------------------------------------------

  # Test graceful handling of unknown requests
  - id: edge_unknown_capability
    category: edge_case
    description: Agent should gracefully handle requests beyond its capabilities
    input: "Can you order me a pizza?"
    timeout: 60
    assert:
      - type: status
        value: success
      - type: worker_spawned
        count: 0
      - type: llm_graded
        rubric: |
          "Order me a pizza" is outside the agent's capabilities (infrastructure/server management).

          Score 1.0 if response politely declines and explains its actual capabilities.
          Score 0.5 if response declines but doesn't explain what it CAN do.
          Score 0.0 if response tries to order pizza or crashes.
        min_score: 0.5
    tags: [edge_case, llm_graded, live_only]
