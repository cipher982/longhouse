# Production Docker Compose - Optimized for production deployment (SQLite)
# Use: docker compose -f docker/docker-compose.prod.yml up

# Production-specific logging settings
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

services:
  # Backend API Service (SQLite)
  backend:
    build:
      context: .
      dockerfile: docker/backend.dockerfile
      target: production
      args:
        BUILD_ENV: production
    restart: unless-stopped
    environment:
      ENVIRONMENT: production
      MODELS_CONFIG_PATH: /config/models.json
      # SQLite database in persistent volume
      DATABASE_URL: sqlite:////data/longhouse.db
      JWT_SECRET: ${JWT_SECRET}
      FERNET_SECRET: ${FERNET_SECRET}
      TRIGGER_SIGNING_SECRET: ${TRIGGER_SIGNING_SECRET}
      AUTH_DISABLED: ${AUTH_DISABLED:-0}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ALLOWED_CORS_ORIGINS: ${ALLOWED_CORS_ORIGINS}
      LLM_TOKEN_STREAM: ${LLM_TOKEN_STREAM:-true}
      DEV_ADMIN: ${DEV_ADMIN:-0}
      # Job queue (durable job execution)
      JOB_QUEUE_ENABLED: ${JOB_QUEUE_ENABLED:-0}
      # Runner seeding (opt-in for production)
      SEED_RUNNERS: ${SEED_RUNNERS:-0}
      # Bootstrap API token for CLI-based config seeding
      BOOTSTRAP_TOKEN: ${BOOTSTRAP_TOKEN}
      # Internal API secret for backend-to-backend calls
      INTERNAL_API_SECRET: ${INTERNAL_API_SECRET}
    volumes:
      - backend_static:/app/static
      # Writable data directory (container root is read-only) - includes SQLite database
      - backend_data:/data
      # SSH key access for worker infrastructure tools (ssh_exec)
      - ${HOME}/.ssh:/home/zerg/.ssh:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - zerg-network
    logging: *default-logging
    # Security: read-only root filesystem
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Frontend Service - React UI (switched from Rust/WASM)
  frontend:
    build:
      context: .
      dockerfile: apps/zerg/frontend-web/Dockerfile
      target: production
      args:
        VITE_UMAMI_WEBSITE_ID: ${VITE_UMAMI_WEBSITE_ID}
        VITE_UMAMI_SCRIPT_SRC: ${VITE_UMAMI_SCRIPT_SRC}
        VITE_UMAMI_DOMAINS: ${VITE_UMAMI_DOMAINS}
    restart: unless-stopped
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:80/ | grep -q 'Longhouse'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - zerg-network
    logging: *default-logging
    # Security: read-only root filesystem
    read_only: true
    tmpfs:
      - /var/cache/nginx:noexec,nosuid,size=50m,uid=1000,gid=1000
      - /var/log/nginx:noexec,nosuid,size=10m,uid=1000,gid=1000
      - /var/run:noexec,nosuid,size=10m,uid=1000,gid=1000
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'

  # Reverse Proxy - Routes paths to internal services
  # Coolify/Caddy handles SSL, this handles path-based routing
  reverse-proxy:
    build:
      context: .
      dockerfile: docker/nginx.dockerfile
    restart: unless-stopped
    depends_on:
      - frontend
      - backend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - zerg-network
    logging: *default-logging

networks:
  zerg-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  backend_static:
    driver: local
  backend_data:
    driver: local
